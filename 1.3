import requests
from bs4 import BeautifulSoup
from fake_headers import Headers
import urllib.parse
import re

query = {input('Введите название вакансии: '): ''} #запрашиваем у пользователя интересующую вакансию
print('Заявка обрабатывается, пожалуйста подождите...')

# поскольку опрос занимает продолжительное время - создаём форму для отмены
while True:
    stuff = input('(...or enter "e" to exit)')
    if stuff in ['e', 'E', 'е', 'Е']:
        print('Задача отменена')
        break

    params = urllib.parse.urlencode(query)  # декодируем текст введённый текст в URL-Uinikod
    # Создаём ссылки для обащения парсера
    sj_list_url = ''.join(['https://yalta.superjob.ru/vacancy/search/?keywords=', params[0:-1:1]])
    hh_list_url = ''.join(['https://hh.ru/search/vacancy?fromSearchLine=true&st=searchVacancy&text=', params[0:-1:1]])

    # объявляем переменные для хранения результатов
    result_categories = ["Вакансия:", "Компания:", "Местоположение:", "Зарплата:", "Ссылка:"]
    result_list = []

#Контрольная ссылка по запросу "Программист" на сайте SJ
#https://yalta.superjob.ru/vacancy/search/?keywords=%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%81%D1%82
#Контрольная ссылка по запросу "Программист" на сайте hh
# hh_list_url = 'https://hh.ru/search/vacancy?fromSearchLine=true&st=searchVacancy&text=%D0%9F%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B8%D1%81%D1%82'

# -------------------------------------------- Блок парсинга SuperJob -------------------------------------------------------

# Функция сбора информации со страницы вакансии
    def get_sjposition_info(position_url): # Функция получения информации со страницы вакансии SJ
        posishion_content = requests.get(position_url)
        posishion_soup = BeautifulSoup(posishion_content.text, 'lxml')
        city_div = posishion_soup.find(attrs={'class': "f-test-address"}).next.text
        zp = posishion_soup.find(attrs={'class': "ZON4b"}).text.replace('\xa0', ' ')
        company_name = posishion_soup.select_one('._3zucV .icMQ_ h2').text.replace('\xa0', ' ')

        return (city_div, zp, company_name)

# Парсинг результатов поиска по интересующей вакансии
    sj_response = requests.get(sj_list_url).text
    sj_soup = BeautifulSoup(sj_response, 'lxml')
    sj_vacancy = sj_soup.find_all(attrs={'class': "f-test-vacancy-item"})  # Вытаскиваем все карточки вакансий из выдачи

# Цикл извлечения параметров вакансии из собранных карточек
    for id, vacant in enumerate(sj_vacancy):
        #ищем данные о названии вакансии, ссылке на её страницу. Город, зарплата и названия компании извлекаются через функцию get_sjposition_info()
        posishion = vacant.find(attrs={'class': re.compile("f-test-link-") }).text
        vacancy_href = vacant.find(attrs={'href': re.compile(".html")})
        # id = str(vacancy_href['href'])[-6:-14:-1][::-1] # Рудимент оставлен как возможность для расширения функционала
        posishion_url = str(''.join(['https://superjob.ru', vacancy_href['href']]))
        city = get_sjposition_info(posishion_url)[0]
        zp = get_sjposition_info(posishion_url)[1]
        company_name = get_sjposition_info(posishion_url)[2]

        position_desc = [posishion, company_name, city, zp, posishion_url]
        result_list.append(position_desc) #Передаём параметры на храненение

# -------------------------------------------- Блок парсинга с hh.ru -------------------------------------------------------
    print("Половину закончили..") #Ободряющая надпись

# Парсинг результатов поиска по интересующей вакансии
    header = Headers(headers=True).generate()
    hh_response = requests.get(hh_list_url, headers=header)
    hh_soup = BeautifulSoup(hh_response.text, 'lxml')
    hh_vacancy = hh_soup.find_all(attrs={'class': "vacancy-serp-item"})  # Вытаскиваем содержимое всех карточек вакансий

    for id, vacant in enumerate(hh_vacancy):
        #ищем данные о названии вакансии, ссылке на её страницу, города, заработной платы
        posishion = vacant.find(attrs={'class': 'vacancy-serp-item__info'}).text
        vacancy_href = vacant.find(attrs={'href': re.compile("https://hh.ru/vacancy/")})
        posishion_url = str(vacancy_href['href'])
        city = vacant.find('span', attrs={'class': 'vacancy-serp-item__meta-info'}).text
        zp = vacant.find(attrs={'class': 'vacancy-serp-item__sidebar'}).text.replace('\u202f', ' ')

        # Название компании вытаскиваем со страницы вакансии
        posishion_content = requests.get(posishion_url, headers=header)
        posishion_soup = BeautifulSoup(posishion_content.text, 'lxml')
        company_name = posishion_soup.find(attrs={'class': 'vacancy-company-name'}).text.replace('\xa0', ' ')

        position_desc = [posishion, company_name, city, zp, posishion_url]
        result_list.append(position_desc) #Передаём параметры на храненение


# Формируем цикл выдачи результата

    for i in result_list:
        print(list(zip({result_categories: i})))
        print('\n')

